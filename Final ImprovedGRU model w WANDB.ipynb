{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Imports and Setup\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "nltk.download('punkt_tab', force=True)   # will re-download even if it thinks it's there\n",
        "nltk.download('punkt', force=True)\n",
        "\n",
        "# Download NLTK resources if needed\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvSvY__y5M8H",
        "outputId": "7aa5ef0a-d995-49a1-9007-625fb74aa3d2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -q\n",
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikVpXFx-NcIn",
        "outputId": "b82d6cf6-965e-414f-ba1b-302090e4d381"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Create a new API key at: https://wandb.ai/authorize?ref=models\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Store your API key securely and do not share it.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste your API key and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjarrennsb\u001b[0m (\u001b[33mjarrennsb-abc\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load Data\n",
        "# Assuming data is in 'spam_assassin.csv' - adjust path if needed\n",
        "df = pd.read_csv(\"spam_assassin.csv\", encoding=\"utf-8\")\n",
        "y = df[\"target\"].values\n",
        "print(df.info())\n",
        "print(df.head())\n",
        "\n",
        "# Class distribution\n",
        "print(df[\"target\"].value_counts())\n",
        "\n",
        "# Length features (optional, not used in model)\n",
        "df[\"char_len\"] = df[\"text\"].astype(str).str.len()\n",
        "df[\"word_len\"] = df[\"text\"].astype(str).apply(lambda x: len(x.split()))\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Sghm2se5N6F",
        "outputId": "b5803079-bfef-4bc5-f10e-702ccacd0da8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5796 entries, 0 to 5795\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   text    5796 non-null   object\n",
            " 1   target  5796 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 90.7+ KB\n",
            "None\n",
            "                                                text  target\n",
            "0  From ilug-admin@linux.ie Mon Jul 29 11:28:02 2...       0\n",
            "1  From gort44@excite.com Mon Jun 24 17:54:21 200...       1\n",
            "2  From fork-admin@xent.com Mon Jul 29 11:39:57 2...       1\n",
            "3  From dcm123@btamail.net.cn Mon Jun 24 17:49:23...       1\n",
            "4  From ilug-admin@linux.ie Mon Aug 19 11:02:47 2...       0\n",
            "target\n",
            "0    3900\n",
            "1    1896\n",
            "Name: count, dtype: int64\n",
            "                                                text  target  char_len  \\\n",
            "0  From ilug-admin@linux.ie Mon Jul 29 11:28:02 2...       0      4098   \n",
            "1  From gort44@excite.com Mon Jun 24 17:54:21 200...       1      2195   \n",
            "2  From fork-admin@xent.com Mon Jul 29 11:39:57 2...       1      3600   \n",
            "3  From dcm123@btamail.net.cn Mon Jun 24 17:49:23...       1      1946   \n",
            "4  From ilug-admin@linux.ie Mon Aug 19 11:02:47 2...       0      3068   \n",
            "\n",
            "   word_len  \n",
            "0       558  \n",
            "1       295  \n",
            "2       386  \n",
            "3       153  \n",
            "4       399  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Improved Text Cleaning Function\n",
        "def clean_text(text):\n",
        "    # 1. Decode HTML entities & remove tags\n",
        "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
        "\n",
        "    # 2. DETECT PHISHING PATTERNS FIRST (before URL extraction!)\n",
        "    def detect_phishing_patterns(text):\n",
        "        \"\"\"Detect common phishing patterns\"\"\"\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        # Phishing keywords\n",
        "        phishing_words = ['verify', 'suspended', 'confirm', 'urgent', 'action required',\n",
        "                          'update payment', 'security alert', 'unusual activity']\n",
        "\n",
        "        # Check for phishing + URL combination\n",
        "        has_url = bool(re.search(r'http[s]?://|www\\.', text))\n",
        "        has_phishing_word = any(word in text_lower for word in phishing_words)\n",
        "\n",
        "        # If phishing word + URL, add token\n",
        "        if has_url and has_phishing_word:\n",
        "            text = 'PHISHING ' + text\n",
        "\n",
        "        return text\n",
        "\n",
        "    # ‚úÖ RUN PHISHING DETECTION BEFORE URL EXTRACTION\n",
        "    text = detect_phishing_patterns(text)\n",
        "\n",
        "    # 3. Extract URL domain features (AFTER phishing detection)\n",
        "    def extract_url_features(text):\n",
        "        \"\"\"Extract domain components instead of replacing with generic <URL>\"\"\"\n",
        "        urls = re.findall(r'(http[s]?://[^\\s]+|www\\.[^\\s]+)', text, flags=re.IGNORECASE)\n",
        "\n",
        "        for url in urls:\n",
        "            domain_match = re.search(r'(?:http[s]?://)?([^/\\s]+)', url)\n",
        "\n",
        "            if domain_match:\n",
        "                domain = domain_match.group(1)\n",
        "                domain = re.sub(r'^www\\.', '', domain, flags=re.IGNORECASE)\n",
        "                domain_parts = domain.replace('.', ' ').replace('-', ' ')\n",
        "                replacement = f' url {domain_parts} '\n",
        "                text = text.replace(url, replacement)\n",
        "\n",
        "        return text\n",
        "\n",
        "    text = extract_url_features(text)\n",
        "\n",
        "    # 4. Email and phone replacement\n",
        "    text = re.sub(r'\\S+@\\S+', 'EMAIL', text)\n",
        "    text = re.sub(r'\\b(?:\\+?\\d{1,3}[-.\\s]?)?(?:\\(\\d{3}\\)|\\d{3})[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b', ' <PHONE> ', text)\n",
        "\n",
        "    # 5. Detect large money amounts\n",
        "    def detect_large_amounts(text):\n",
        "        amounts = re.findall(r'\\$[\\d,]+', text)\n",
        "        for amount in amounts:\n",
        "            num_str = amount.replace('$', '').replace(',', '')\n",
        "            if num_str.isdigit():\n",
        "                num = int(num_str)\n",
        "                if num >= 10000:\n",
        "                    text = text.replace(amount, ' XXLARGEMONEY ')\n",
        "                elif num >= 100:\n",
        "                    text = text.replace(amount, ' XXMONEY ')\n",
        "        return text\n",
        "\n",
        "    text = detect_large_amounts(text)\n",
        "\n",
        "    # 6. Detect phishing patterns\n",
        "    def detect_phishing_patterns(text):\n",
        "        text_lower = text.lower()\n",
        "        phishing_words = ['verify', 'suspended', 'confirm', 'urgent', 'action required',\n",
        "                          'update payment', 'security alert', 'unusual activity']\n",
        "        has_url = bool(re.search(r'http[s]?://|www\\.', text))\n",
        "        has_phishing_word = any(word in text_lower for word in phishing_words)\n",
        "\n",
        "        if has_url and has_phishing_word:\n",
        "            text = 'PHISHING ' + text  # ‚úÖ No brackets!\n",
        "\n",
        "        return text\n",
        "\n",
        "    text = detect_phishing_patterns(text)\n",
        "\n",
        "\n",
        "\n",
        "    # 7. Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # 7a. Replace placeholders with lowercase tokens\n",
        "    text = text.replace('xxlargemoney', 'largemoney')\n",
        "    text = text.replace('xxmoney', 'money')\n",
        "\n",
        "    # 7b. Phishing detection AFTER lowercasing\n",
        "    phishing_words = ['verify', 'suspended', 'confirm', 'urgent', 'action required',\n",
        "                      'update payment', 'security alert', 'unusual activity']\n",
        "    has_url = bool(re.search(r'url', text))  # Already processed\n",
        "    has_phishing_word = any(word in text for word in phishing_words)\n",
        "\n",
        "    if has_url and has_phishing_word:\n",
        "        text = 'phishing ' + text  # lowercase token with space\n",
        "\n",
        "    # 8. ‚úÖ NEW: Prize scam detection\n",
        "    prize_words = ['won', 'win', 'winner', 'prize', 'congratulations', 'claim']\n",
        "    has_large_money = 'largemoney' in text\n",
        "    has_prize_word = any(word in text for word in prize_words)\n",
        "\n",
        "    if has_large_money and has_prize_word:\n",
        "        text = 'prizescam ' + text  # ‚úÖ Add prize scam token!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # 9. Normalize repeated punctuation\n",
        "    text = re.sub(r'([!?.]){3,}', r'\\1\\1\\1', text)\n",
        "\n",
        "    # 10. Collapse multiple spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply cleaning\n",
        "df[\"clean_text\"] = df[\"text\"].astype(str).apply(clean_text)\n",
        "print(\"\\nSample cleaned text:\\n\", df[\"clean_text\"][0][:500], \"...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeIwsRXC9J0F",
        "outputId": "e933b47c-2702-4247-c2f5-553b6380af9f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample cleaned text:\n",
            " from email mon jul 29 11:28:02 2002 return-path: delivered-to: email received: from localhost (localhost [127.0.0.1]) by phobos.labs.netnoteinc.com (postfix) with esmtp id a13d94414f for ; mon, 29 jul 2002 06:25:11 -0400 (edt) received: from phobos [127.0.0.1] by localhost with imap (fetchmail-5.9.0) for email (single-drop); mon, 29 jul 2002 11:25:11 +0100 (ist) received: from lugh.tuatha.org email [194.125.145.45]) by dogma.slashnull.org (8.11.6/8.11.6) with esmtp id g6rhn7i17130 for ; sat, 27  ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Tokenization (Light Stopword Removal - Optional, but recommended light version)\n",
        "stop_words = set(stopwords.words('english')) - {'not', 'no', 'never', 'none'}  # Keep negation words for sentiment/spam\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    # Light stopword removal: Only remove very common ones if desired\n",
        "    # Comment out the next line to skip stopword removal entirely\n",
        "    tokens = [word for word in tokens if word not in list(stop_words)[:50]]  # e.g., top 50 stopwords only\n",
        "    return tokens\n",
        "\n",
        "# Build vocabulary\n",
        "all_tokens = []\n",
        "for text in df[\"clean_text\"]:\n",
        "    all_tokens.extend(tokenize(text))\n",
        "\n",
        "word_counts = Counter(all_tokens)\n",
        "vocab_size = 15000\n",
        "vocab = sorted(word_counts, key=word_counts.get, reverse=True)[:vocab_size-7]  # Reserve for specials\n",
        "\n",
        "specials = [\"<PAD>\", \"<UNK>\", \"<START>\",\"phishing\", \"largemoney\", \"money\", \"prizescam\"]\n",
        "stoi = {word: idx + len(specials) for idx, word in enumerate(vocab)}\n",
        "stoi.update({special: idx for idx, special in enumerate(specials)})\n",
        "\n",
        "itos = specials + vocab\n",
        "\n",
        "print(f\"Vocabulary size: {len(stoi)}\")\n",
        "print(\"Sample vocab:\", list(stoi.keys())[:10])\n",
        "\n",
        "# Verify special tokens are in vocabulary\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"VERIFYING SPECIAL TOKENS\")\n",
        "print(\"=\"*70)\n",
        "for token in specials:\n",
        "    if token in stoi:\n",
        "        print(f\"‚úì {token:20s} ‚Üí index {stoi[token]}\")\n",
        "    else:\n",
        "        print(f\"‚úó {token:20s} ‚Üí MISSING!\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4MUs8vk5eZw",
        "outputId": "446fe199-f63a-43f9-88c4-363eefc32531"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 14996\n",
            "Sample vocab: [':', ',', ')', '(', 'email', '--', '.', 'from', '2002', \"''\"]\n",
            "\n",
            "======================================================================\n",
            "VERIFYING SPECIAL TOKENS\n",
            "======================================================================\n",
            "‚úì <PAD>                ‚Üí index 0\n",
            "‚úì <UNK>                ‚Üí index 1\n",
            "‚úì <START>              ‚Üí index 2\n",
            "‚úì phishing             ‚Üí index 3\n",
            "‚úì largemoney           ‚Üí index 4\n",
            "‚úì money                ‚Üí index 5\n",
            "‚úì prizescam            ‚Üí index 6\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check for spam keywords"
      ],
      "metadata": {
        "id": "L18JE4zwROFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spam_keywords = ['win', 'won', 'prize', 'claim', 'free', 'urgent',\n",
        "                 'limited', 'act', 'click', 'verify', 'congratulations']\n",
        "\n",
        "for word in spam_keywords:\n",
        "    if word in stoi:\n",
        "        print(f\"‚úì '{word}' in vocab (index {stoi[word]})\")\n",
        "    else:\n",
        "        print(f\"‚úó '{word}' NOT in vocab\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNfncBxHRHbb",
        "outputId": "09ef13df-1460-44db-aaea-8032e63bbfe4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì 'win' in vocab (index 1511)\n",
            "‚úó 'won' NOT in vocab\n",
            "‚úì 'prize' in vocab (index 3413)\n",
            "‚úì 'claim' in vocab (index 979)\n",
            "‚úì 'free' in vocab (index 113)\n",
            "‚úì 'urgent' in vocab (index 1736)\n",
            "‚úì 'limited' in vocab (index 999)\n",
            "‚úì 'act' in vocab (index 769)\n",
            "‚úì 'click' in vocab (index 167)\n",
            "‚úì 'verify' in vocab (index 2455)\n",
            "‚úì 'congratulations' in vocab (index 3604)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Encoding and Padding\n",
        "max_len = 200\n",
        "\n",
        "def encode(text, stoi, max_len=max_len):\n",
        "    tokens = tokenize(text)\n",
        "    ids = [stoi.get(tok, stoi[\"<UNK>\"]) for tok in tokens]\n",
        "    ids = ids[:max_len]\n",
        "    if len(ids) < max_len:\n",
        "        ids = ids + [stoi[\"<PAD>\"]] * (max_len - len(ids))\n",
        "    return np.array(ids, dtype=np.int64)"
      ],
      "metadata": {
        "id": "O3G3XhYL5gzn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Data Split\n",
        "texts = df[\"clean_text\"].values\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(texts, y, test_size=0.3, stratify=y, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
        "\n",
        "print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
        "\n",
        "X_train_ids = np.stack([encode(t, stoi) for t in X_train])\n",
        "X_val_ids   = np.stack([encode(t, stoi) for t in X_val])\n",
        "X_test_ids  = np.stack([encode(t, stoi) for t in X_test])\n",
        "\n",
        "y_train = y_train.astype(np.float32)\n",
        "y_val   = y_val.astype(np.float32)\n",
        "y_test  = y_test.astype(np.float32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ym-ejEW15ht9",
        "outputId": "4f58202f-836e-4fe0-9584-ddd462c84e56"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 4057, Val: 869, Test: 870\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Datasets and Loaders\n",
        "class SpamDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.from_numpy(X)\n",
        "        self.y = torch.from_numpy(y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "batch_size = 32\n",
        "train_ds = SpamDataset(X_train_ids, y_train)\n",
        "val_ds   = SpamDataset(X_val_ids, y_val)\n",
        "test_ds  = SpamDataset(X_test_ids, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
        "test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "qlacCQIY5kfA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Model Definition\n",
        "class ImprovedSPAMGRU(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=256, hidden_size=256, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "\n",
        "        # Bidirectional GRU\n",
        "        self.gru = nn.GRU(\n",
        "            embed_dim,\n",
        "            hidden_size,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.attention = nn.Linear(hidden_size * 2, 1)\n",
        "\n",
        "        # Classification layers\n",
        "        self.fc1 = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc2 = nn.Linear(hidden_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Embedding\n",
        "        embedded = self.embedding(x)\n",
        "\n",
        "        # GRU\n",
        "        gru_out, _ = self.gru(embedded)\n",
        "\n",
        "        # Attention\n",
        "        attention_weights = torch.softmax(self.attention(gru_out), dim=1)\n",
        "        context = torch.sum(attention_weights * gru_out, dim=1)\n",
        "\n",
        "        # Classification\n",
        "        x = self.fc1(context)\n",
        "        x = torch.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return self.sigmoid(x).squeeze()"
      ],
      "metadata": {
        "id": "GNurz7-Q5niv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Training Configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "EMBED_DIM = 256\n",
        "HIDDEN_SIZE = 256\n",
        "DROPOUT = 0.3\n",
        "NUM_EPOCHS = 10\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "model = ImprovedSPAMGRU(\n",
        "    vocab_size=len(stoi),\n",
        "    embed_dim=EMBED_DIM,\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    dropout=DROPOUT\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Training function\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for texts, labels in train_loader:\n",
        "        texts, labels = texts.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(texts)\n",
        "        loss = criterion(outputs, labels.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in val_loader:\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "            outputs = model(texts)\n",
        "            loss = criterion(outputs, labels.float())\n",
        "            total_loss += loss.item()\n",
        "            predictions = (outputs >= 0.5).long()\n",
        "            correct += (predictions == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    accuracy = correct / total\n",
        "    avg_loss = total_loss / len(val_loader)\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9WCYW5U5qnd",
        "outputId": "7ca16aee-6a2a-4f25-8529-69e6781f2a33"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize W&B project\n",
        "wandb.init(\n",
        "    project=\"spam-detection\",\n",
        "    name=\"ImprovedSPAMGRU-BiGRU\",\n",
        "    config={\n",
        "        \"epochs\": NUM_EPOCHS,\n",
        "        \"learning_rate\": LEARNING_RATE,\n",
        "        \"vocab_size\": len(stoi),\n",
        "        \"embed_dim\": EMBED_DIM,\n",
        "        \"hidden_size\": HIDDEN_SIZE,\n",
        "        \"dropout\": DROPOUT,\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"‚úÖ W&B initialized! View your run at:\", wandb.run.get_url())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "1x_CXS2sOLti",
        "outputId": "d6264405-65c6-4c67-df18-906ac37cfb0b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.25.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20260222_023840-krmijcsp</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jarrennsb-abc/spam-detection/runs/krmijcsp' target=\"_blank\">ImprovedSPAMGRU-BiGRU</a></strong> to <a href='https://wandb.ai/jarrennsb-abc/spam-detection' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jarrennsb-abc/spam-detection' target=\"_blank\">https://wandb.ai/jarrennsb-abc/spam-detection</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jarrennsb-abc/spam-detection/runs/krmijcsp' target=\"_blank\">https://wandb.ai/jarrennsb-abc/spam-detection/runs/krmijcsp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The get_url method is deprecated and will be removed in a future release. Please use `run.url` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ W&B initialized! View your run at: https://wandb.ai/jarrennsb-abc/spam-detection/runs/krmijcsp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Training Loop\n",
        "print(\"=\" * 70)\n",
        "print(\"TRAINING IMPROVEDSPAMGRU MODEL\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "best_val_acc = 0\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "    # ADDED LINE - Log to W&B\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"train_loss\": train_loss,\n",
        "        \"val_loss\": val_loss,\n",
        "        \"val_accuracy\": val_acc\n",
        "    })\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
        "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
        "    print(f\"  Val Accuracy: {val_acc:.4f}\")\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), 'spam_model.pth')\n",
        "        print(\"  ‚úì New best model saved!\")\n",
        "    print()\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAdKbBNY5tR4",
        "outputId": "7e8f32ab-e078-49aa-f636-e2690e5d7ad1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "TRAINING IMPROVEDSPAMGRU MODEL\n",
            "======================================================================\n",
            "Epoch 1/10\n",
            "  Train Loss: 0.2297\n",
            "  Val Loss: 0.1108\n",
            "  Val Accuracy: 0.9586\n",
            "  ‚úì New best model saved!\n",
            "\n",
            "Epoch 2/10\n",
            "  Train Loss: 0.0822\n",
            "  Val Loss: 0.1464\n",
            "  Val Accuracy: 0.9379\n",
            "\n",
            "Epoch 3/10\n",
            "  Train Loss: 0.0396\n",
            "  Val Loss: 0.0510\n",
            "  Val Accuracy: 0.9850\n",
            "  ‚úì New best model saved!\n",
            "\n",
            "Epoch 4/10\n",
            "  Train Loss: 0.0241\n",
            "  Val Loss: 0.0417\n",
            "  Val Accuracy: 0.9839\n",
            "\n",
            "Epoch 5/10\n",
            "  Train Loss: 0.0203\n",
            "  Val Loss: 0.0472\n",
            "  Val Accuracy: 0.9781\n",
            "\n",
            "Epoch 6/10\n",
            "  Train Loss: 0.0107\n",
            "  Val Loss: 0.0357\n",
            "  Val Accuracy: 0.9827\n",
            "\n",
            "Epoch 7/10\n",
            "  Train Loss: 0.0126\n",
            "  Val Loss: 0.0335\n",
            "  Val Accuracy: 0.9873\n",
            "  ‚úì New best model saved!\n",
            "\n",
            "Epoch 8/10\n",
            "  Train Loss: 0.0023\n",
            "  Val Loss: 0.0415\n",
            "  Val Accuracy: 0.9827\n",
            "\n",
            "Epoch 9/10\n",
            "  Train Loss: 0.0052\n",
            "  Val Loss: 0.0571\n",
            "  Val Accuracy: 0.9839\n",
            "\n",
            "Epoch 10/10\n",
            "  Train Loss: 0.0149\n",
            "  Val Loss: 0.0641\n",
            "  Val Accuracy: 0.9804\n",
            "\n",
            "======================================================================\n",
            "TRAINING COMPLETE!\n",
            "Best Validation Accuracy: 0.9873\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11: Test Evaluation and Classification Report\n",
        "model.load_state_dict(torch.load('spam_model.pth'))\n",
        "test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "\n",
        "print(\"\\nFinal Test Results:\")\n",
        "print(f\"  Test Loss: {test_loss:.4f}\")\n",
        "print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Log training loss and accuracy\n",
        "wandb.summary[\"test_loss\"] = test_loss\n",
        "wandb.summary[\"test_accuracy\"] = test_acc\n",
        "\n",
        "# Detailed classification report\n",
        "model.eval()\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for texts, labels in test_loader:\n",
        "        texts = texts.to(device)\n",
        "        outputs = model(texts)\n",
        "        predictions = (outputs >= 0.5).long().cpu().numpy()\n",
        "        all_predictions.extend(predictions)\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"CLASSIFICATION REPORT\")\n",
        "print(\"=\" * 70)\n",
        "print(classification_report(all_labels, all_predictions,\n",
        "                            target_names=['Ham (Non-Spam)', 'Spam'],\n",
        "                            digits=4))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"CONFUSION MATRIX\")\n",
        "print(\"=\" * 70)\n",
        "cm = confusion_matrix(all_labels, all_predictions)\n",
        "print(f\"                  Predicted\")\n",
        "print(f\"                  Ham    Spam\")\n",
        "print(f\"Actual Ham     {cm[0][0]:6d} {cm[0][1]:6d}\")\n",
        "print(f\"       Spam    {cm[1][0]:6d} {cm[1][1]:6d}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Model summary\n",
        "print(\"\\nModel Architecture:\")\n",
        "print(model)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"\\nTotal Parameters: {total_params:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfvQmf3u5v1g",
        "outputId": "48510d91-4c0d-4022-f6c5-4dfcfeb253ae"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Test Results:\n",
            "  Test Loss: 0.0367\n",
            "  Test Accuracy: 0.9874\n",
            "\n",
            "======================================================================\n",
            "CLASSIFICATION REPORT\n",
            "======================================================================\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "Ham (Non-Spam)     0.9983    0.9829    0.9905       585\n",
            "          Spam     0.9660    0.9965    0.9810       285\n",
            "\n",
            "      accuracy                         0.9874       870\n",
            "     macro avg     0.9821    0.9897    0.9858       870\n",
            "  weighted avg     0.9877    0.9874    0.9874       870\n",
            "\n",
            "\n",
            "======================================================================\n",
            "CONFUSION MATRIX\n",
            "======================================================================\n",
            "                  Predicted\n",
            "                  Ham    Spam\n",
            "Actual Ham        575     10\n",
            "       Spam         1    284\n",
            "======================================================================\n",
            "\n",
            "Model Architecture:\n",
            "ImprovedSPAMGRU(\n",
            "  (embedding): Embedding(14996, 256, padding_idx=0)\n",
            "  (gru): GRU(256, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
            "  (attention): Linear(in_features=512, out_features=1, bias=True)\n",
            "  (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc2): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "\n",
            "Total Parameters: 5,943,298\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Log final test results to W&B\n",
        "wandb.log({\n",
        "    \"test_loss\": test_loss,\n",
        "    \"test_accuracy\": test_acc\n",
        "})\n",
        "\n",
        "print(f\"‚úÖ Test results logged to W&B!\")\n",
        "print(f\"   Test Loss: {test_loss:.4f}\")\n",
        "print(f\"   Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHL1HIWqPyFb",
        "outputId": "1082f780-cd59-4bbc-fa31-f7adb9bfe19b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Test results logged to W&B!\n",
            "   Test Loss: 0.0367\n",
            "   Test Accuracy: 0.9874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Finish W&B run\n",
        "wandb.finish()\n",
        "print(\"‚úÖ W&B run finished! Check your dashboard at: https://wandb.ai\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "a9jd203pP6XC",
        "outputId": "0f8d69b9-9203-45f7-9451-a9556c8848aa"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà</td></tr><tr><td>test_accuracy</td><td>‚ñÅ</td></tr><tr><td>test_loss</td><td>‚ñÅ</td></tr><tr><td>train_loss</td><td>‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_accuracy</td><td>‚ñÑ‚ñÅ‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá</td></tr><tr><td>val_loss</td><td>‚ñÜ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>test_accuracy</td><td>0.98736</td></tr><tr><td>test_loss</td><td>0.0367</td></tr><tr><td>train_loss</td><td>0.01487</td></tr><tr><td>val_accuracy</td><td>0.98044</td></tr><tr><td>val_loss</td><td>0.06408</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">ImprovedSPAMGRU-BiGRU</strong> at: <a href='https://wandb.ai/jarrennsb-abc/spam-detection/runs/krmijcsp' target=\"_blank\">https://wandb.ai/jarrennsb-abc/spam-detection/runs/krmijcsp</a><br> View project at: <a href='https://wandb.ai/jarrennsb-abc/spam-detection' target=\"_blank\">https://wandb.ai/jarrennsb-abc/spam-detection</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20260222_023840-krmijcsp/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ W&B run finished! Check your dashboard at: https://wandb.ai\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 12: Save Model and Config for Deployment\n",
        "\n",
        "config = {\n",
        "    'vocab_size': len(stoi),\n",
        "    'embed_dim': EMBED_DIM,\n",
        "    'hidden_size': HIDDEN_SIZE,\n",
        "    'dropout': DROPOUT,\n",
        "    'max_len': max_len,\n",
        "    'stoi': stoi\n",
        "}\n",
        "torch.save(config, 'spam_config.pth')\n",
        "print(\"‚úì Model and configuration saved!\")\n",
        "print(\"  - spam_model.pth\")\n",
        "print(\"  - spam_config.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEaTrVNQ5zXX",
        "outputId": "38a4f855-e868-4b80-bf02-e2c2c338608e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Model and configuration saved!\n",
            "  - spam_model.pth\n",
            "  - spam_config.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# UPDATED DIAGNOSTIC TEST\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import torch\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"COMPREHENSIVE MODEL DIAGNOSTIC TEST\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Test 1: Check if special tokens are in vocabulary\n",
        "print(\"\\nüìö TEST 1: Special Tokens in Vocabulary\")\n",
        "print(\"-\"*80)\n",
        "special_tokens_needed = ['phishing', 'largemoney', 'money','prizescam']  # ‚úÖ Updated names\n",
        "missing_tokens = []\n",
        "\n",
        "for token in special_tokens_needed:\n",
        "    if token in stoi:\n",
        "        print(f\"‚úì {token:25s} ‚Üí index {stoi[token]}\")\n",
        "    else:\n",
        "        print(f\"‚úó {token:25s} ‚Üí MISSING IN VOCABULARY!\")\n",
        "        missing_tokens.append(token)\n",
        "\n",
        "if missing_tokens:\n",
        "    print(f\"\\n‚ùå CRITICAL: {len(missing_tokens)} special tokens missing!\")\n",
        "    print(\"   These tokens will be mapped to <UNK> and model can't learn from them!\")\n",
        "    print(\"   FIX: Update Cell 3 to include these in specials list, then RETRAIN\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ All special tokens present in vocabulary\")\n",
        "\n",
        "# Test 2: Check preprocessing on spam examples\n",
        "print(\"\\n\\nüßπ TEST 2: Preprocessing Output\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "test_examples = [\n",
        "    (\"Prize scam\", \"You've just won $2,500,000! Claim now: http://luckyclaim.net\"),\n",
        "    (\"Phishing\", \"URGENT: Account suspended! Verify now: http://secure-bank-login.xyz\"),\n",
        "    (\"Free offer\", \"CONGRATULATIONS!!! FREE iPhone 15! Click: http://winner-prize.com LIMITED TIME!!!\")\n",
        "]\n",
        "\n",
        "for name, text in test_examples:\n",
        "    cleaned = clean_text(text)\n",
        "    tokens = tokenize(cleaned)\n",
        "\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Cleaned: {cleaned[:100]}...\")\n",
        "    print(f\"  First tokens: {tokens[:10]}\")\n",
        "\n",
        "    # ‚úÖ Check for NEW token names (no brackets!)\n",
        "    has_phishing = 'phishing' in cleaned.lower()\n",
        "    has_large_money = 'largemoney' in cleaned.lower()\n",
        "    has_money = 'money' in cleaned.lower() and 'largemoney' not in cleaned.lower()\n",
        "\n",
        "    signals = []\n",
        "    if has_phishing:\n",
        "        signals.append(\"üé£ PHISHING\")\n",
        "    if has_large_money:\n",
        "        signals.append(\"üí∞ LARGEMONEY\")\n",
        "    if has_money:\n",
        "        signals.append(\"üíµ MONEY\")\n",
        "\n",
        "    if signals:\n",
        "        print(f\"  Signals: {', '.join(signals)}\")\n",
        "    else:\n",
        "        print(f\"  ‚ùå NO SPECIAL TOKENS DETECTED!\")\n",
        "\n",
        "# Test 3: Encode and check indices\n",
        "print(\"\\n\\nüî¢ TEST 3: Token Encoding\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "test_text = \"You've just won $2,500,000! Claim now: http://luckyclaim.net\"\n",
        "cleaned = clean_text(test_text)\n",
        "tokens = tokenize(cleaned)\n",
        "encoded = encode(cleaned, stoi)\n",
        "\n",
        "print(f\"Example: {test_text[:60]}...\")\n",
        "print(f\"Cleaned: {cleaned[:80]}...\")\n",
        "print(f\"Tokens: {tokens[:10]}\")\n",
        "print(f\"Encoded (first 15): {encoded[:15]}\")\n",
        "\n",
        "# ‚úÖ Check for NEW special token indices\n",
        "special_indices = {stoi.get(t, -1) for t in ['phishing', 'largemoney', 'money']}\n",
        "found_specials = [idx for idx in encoded[:20] if idx in special_indices and idx != -1]\n",
        "\n",
        "if found_specials:\n",
        "    print(f\"‚úì Special token indices found: {found_specials}\")\n",
        "else:\n",
        "    print(f\"‚ùå NO special token indices in encoded sequence!\")\n",
        "\n",
        "# Test 4: Test model prediction\n",
        "print(\"\\n\\nü§ñ TEST 4: Model Predictions\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "test_cases = [\n",
        "    (\"Prize scam\", \"You've just won $2,500,000! Claim now: http://luckyclaim.net\", \"SPAM\"),\n",
        "    (\"Phishing\", \"URGENT: Account suspended! Verify now: http://secure-bank-login.xyz\", \"SPAM\"),\n",
        "    (\"Free iPhone\", \"CONGRATULATIONS!!! FREE iPhone 15! Click: http://winner-prize.com LIMITED TIME!!!\", \"SPAM\"),\n",
        "    (\"Legitimate\", \"Meeting tomorrow at 2 PM. Please review the documents.\", \"HAM\")\n",
        "]\n",
        "\n",
        "correct = 0\n",
        "total = len(test_cases)\n",
        "\n",
        "for name, text, expected in test_cases:\n",
        "    cleaned = clean_text(text)\n",
        "    encoded = encode(cleaned, stoi)\n",
        "    x = torch.tensor([encoded]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        prob = model(x).item()\n",
        "\n",
        "    predicted = \"SPAM\" if prob >= 0.5 else \"HAM\"\n",
        "    is_correct = (predicted == expected)\n",
        "\n",
        "    symbol = \"‚úì\" if is_correct else \"‚úó\"\n",
        "    if is_correct:\n",
        "        correct += 1\n",
        "\n",
        "    print(f\"{symbol} {name:15s} Expected: {expected:4s}, Got: {predicted:4s} ({prob*100:5.1f}% spam)\")\n",
        "\n",
        "print(f\"\\nAccuracy on test cases: {correct}/{total} ({correct/total*100:.0f}%)\")\n",
        "\n",
        "if correct < 3:\n",
        "    print(\"\\n‚ùå CRITICAL FAILURE: Model performing poorly!\")\n",
        "    print(\"   Most likely causes:\")\n",
        "    print(\"   1. Special tokens not in vocabulary (check TEST 1)\")\n",
        "    print(\"   2. Using old model (not retrained after fixing preprocessing)\")\n",
        "    print(\"   3. Need more training data with these spam types\")\n",
        "\n",
        "# Test 5: Check saved config\n",
        "print(\"\\n\\nüíæ TEST 5: Saved Configuration\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "if os.path.exists('spam_config.pth'):\n",
        "    saved_config = torch.load('spam_config.pth', map_location='cpu')\n",
        "\n",
        "    print(f\"‚úì Config file exists\")\n",
        "    print(f\"  Vocab size: {len(saved_config['stoi'])}\")\n",
        "\n",
        "    # ‚úÖ Check for NEW token names\n",
        "    saved_specials = [t for t in ['phishing', 'largemoney', 'money', 'prizescam']\n",
        "                      if t in saved_config['stoi']]\n",
        "\n",
        "    if len(saved_specials) == 4:\n",
        "        print(f\"‚úì All 4 special tokens in saved config\")\n",
        "        for token in saved_specials:\n",
        "            print(f\"  {token} ‚Üí index {saved_config['stoi'][token]}\")\n",
        "    else:\n",
        "        print(f\"‚ùå Only {len(saved_specials)}/3 special tokens in saved config!\")\n",
        "        print(f\"   Found: {saved_specials}\")\n",
        "else:\n",
        "    print(\"‚ùå Config file not found!\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DIAGNOSTIC TEST COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if missing_tokens:\n",
        "    print(\"\\nüö® ACTION REQUIRED:\")\n",
        "    print(\"1. Update Cell 3 to include special tokens in vocabulary\")\n",
        "    print(\"2. Restart kernel\")\n",
        "    print(\"3. Run ALL cells again to retrain\")\n",
        "    print(\"4. Download NEW model files\")\n",
        "    print(\"5. Upload to Hugging Face\")\n",
        "elif correct < 3:\n",
        "    print(\"\\nüö® ACTION REQUIRED:\")\n",
        "    print(\"Model not learning spam patterns properly.\")\n",
        "    print(\"Check if training data has enough spam examples of these types.\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ Model looks good! Ready for deployment.\")\n",
        "    print(\"   Download spam_model.pth and spam_config.pth\")\n",
        "    print(\"   Upload to Hugging Face\")\n",
        "\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywbaMgQa6msU",
        "outputId": "317cfeef-328c-4cc3-ddc2-bcbc3cd164fd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "COMPREHENSIVE MODEL DIAGNOSTIC TEST\n",
            "================================================================================\n",
            "\n",
            "üìö TEST 1: Special Tokens in Vocabulary\n",
            "--------------------------------------------------------------------------------\n",
            "‚úì phishing                  ‚Üí index 3\n",
            "‚úì largemoney                ‚Üí index 4\n",
            "‚úì money                     ‚Üí index 5\n",
            "‚úì prizescam                 ‚Üí index 6\n",
            "\n",
            "‚úÖ All special tokens present in vocabulary\n",
            "\n",
            "\n",
            "üßπ TEST 2: Preprocessing Output\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Prize scam:\n",
            "  Cleaned: prizescam you've just won largemoney ! claim now: url luckyclaim net...\n",
            "  First tokens: ['prizescam', 'you', \"'ve\", 'just', 'largemoney', '!', 'claim', ':', 'url', 'luckyclaim']\n",
            "  Signals: üí∞ LARGEMONEY\n",
            "\n",
            "Phishing:\n",
            "  Cleaned: phishing phishing urgent: account suspended! verify now: url secure bank login xyz...\n",
            "  First tokens: ['phishing', 'phishing', 'urgent', ':', 'account', 'suspended', '!', 'verify', ':', 'url']\n",
            "  Signals: üé£ PHISHING\n",
            "\n",
            "Free offer:\n",
            "  Cleaned: congratulations!!! free iphone 15! click: url winner prize com limited time!!!...\n",
            "  First tokens: ['congratulations', '!', '!', '!', 'free', 'iphone', '15', '!', 'click', ':']\n",
            "  ‚ùå NO SPECIAL TOKENS DETECTED!\n",
            "\n",
            "\n",
            "üî¢ TEST 3: Token Encoding\n",
            "--------------------------------------------------------------------------------\n",
            "Example: You've just won $2,500,000! Claim now: http://luckyclaim.net...\n",
            "Cleaned: prizescam you've just won largemoney ! claim now: url luckyclaim net...\n",
            "Tokens: ['prizescam', 'you', \"'ve\", 'just', 'largemoney', '!', 'claim', ':', 'url', 'luckyclaim']\n",
            "Encoded (first 15): [  6  29 233 115   4  39 979   7  40   1 139   0   0   0   0]\n",
            "‚úì Special token indices found: [np.int64(4)]\n",
            "\n",
            "\n",
            "ü§ñ TEST 4: Model Predictions\n",
            "--------------------------------------------------------------------------------\n",
            "‚úó Prize scam      Expected: SPAM, Got: HAM  ( 42.8% spam)\n",
            "‚úì Phishing        Expected: SPAM, Got: SPAM ( 99.9% spam)\n",
            "‚úì Free iPhone     Expected: SPAM, Got: SPAM (100.0% spam)\n",
            "‚úì Legitimate      Expected: HAM , Got: HAM  (  0.0% spam)\n",
            "\n",
            "Accuracy on test cases: 3/4 (75%)\n",
            "\n",
            "\n",
            "üíæ TEST 5: Saved Configuration\n",
            "--------------------------------------------------------------------------------\n",
            "‚úì Config file exists\n",
            "  Vocab size: 14996\n",
            "‚úì All 4 special tokens in saved config\n",
            "  phishing ‚Üí index 3\n",
            "  largemoney ‚Üí index 4\n",
            "  money ‚Üí index 5\n",
            "  prizescam ‚Üí index 6\n",
            "\n",
            "================================================================================\n",
            "DIAGNOSTIC TEST COMPLETE\n",
            "================================================================================\n",
            "\n",
            "‚úÖ Model looks good! Ready for deployment.\n",
            "   Download spam_model.pth and spam_config.pth\n",
            "   Upload to Hugging Face\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3046786870.py:108: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
            "  x = torch.tensor([encoded]).to(device)\n"
          ]
        }
      ]
    }
  ]
}